{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification Decision Trees, SVM, and Naive Bayes"
      ],
      "metadata": {
        "id": "mfKPa6I83Xcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "\n",
        "### **Definition:**\n",
        "\n",
        "**Information Gain (IG)** is a **measure used in Decision Trees** to determine **which feature (attribute)** best splits the data into target classes.\n",
        "It tells us **how much “information” a particular feature gives us about the target variable**.\n",
        "\n",
        "In simple terms —\n",
        "\n",
        "> Information Gain measures the **reduction in uncertainty (entropy)** after splitting the dataset based on a particular feature.\n",
        "\n",
        "\n",
        "### **Mathematical Formula:**\n",
        "\n",
        "IG = Entropy(parent) - Σ ( (n_i / n) * Entropy(child_i) )\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* **Entropy(Parent)** → Uncertainty before the split\n",
        "* **Entropy(Childᵢ)** → Uncertainty after the split\n",
        "* **nᵢ** → Number of samples in the child node\n",
        "* **n** → Total number of samples in the parent node\n",
        "\n",
        "\n",
        "### **Step-by-Step Concept:**\n",
        "\n",
        "1. **Compute Entropy of the Parent Node**\n",
        "\n",
        "   * Entropy measures impurity or disorder in data.\n",
        "   * Formula:\n",
        "     [\n",
        "     Entropy = -p_1 \\log_2(p_1) - p_2 \\log_2(p_2)\n",
        "     ]\n",
        "     where ( p_1, p_2 ) are the probabilities of each class.\n",
        "\n",
        "2. **Split Data on a Feature**\n",
        "\n",
        "   * The dataset is divided based on feature values (like “Yes/No” or numeric ranges).\n",
        "\n",
        "3. **Compute Entropy for Each Child Node**\n",
        "\n",
        "   * Calculate entropy of each subset after the split.\n",
        "\n",
        "4. **Calculate Information Gain (IG)**\n",
        "\n",
        "   * Subtract the weighted average entropy of child nodes from the parent entropy.\n",
        "   * The feature with **highest IG** becomes the **decision node**.\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Suppose you’re building a Decision Tree to predict whether a person **buys a product** based on **Age**.\n",
        "\n",
        "* **Before split (Parent Entropy)** = 0.94\n",
        "* **After split (Weighted Child Entropy)** = 0.60\n",
        "\n",
        "[\n",
        "IG = 0.94 - 0.60 = 0.34\n",
        "]\n",
        "\n",
        "So, **Information Gain = 0.34**, meaning splitting by “Age” reduces uncertainty by 0.34 bits.\n",
        "\n",
        "\n",
        "### **Purpose in Decision Trees:**\n",
        "\n",
        "| **Purpose**           | **Explanation**                                        |\n",
        "| --------------------- | ------------------------------------------------------ |\n",
        "| **Feature Selection** | Chooses the attribute that best separates the data.    |\n",
        "| **Node Splitting**    | Determines where to create decision nodes in the tree. |\n",
        "| **Improves Accuracy** | Higher IG → less impurity → better classification.     |\n",
        "\n"
      ],
      "metadata": {
        "id": "Lj6Btcwi3abI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "\n",
        "\n",
        "### **Introduction:**\n",
        "\n",
        "Both **Gini Impurity** and **Entropy** are measures of **impurity or disorder** used in **Decision Trees** to determine the best feature for splitting data.\n",
        "They quantify how mixed the classes are in a node — a node is **pure** when all data points belong to one class and **impure** when they are spread across multiple classes.\n",
        "\n",
        "\n",
        "### **1. Gini Impurity**\n",
        "\n",
        "**Definition:**\n",
        "Gini Impurity measures the probability that a randomly chosen element from a dataset would be **incorrectly classified** if it was randomly labeled according to the distribution of labels in the subset.\n",
        "\n",
        "**Formula:**\n",
        "[\n",
        "Gini = 1 - \\sum p_i^2\n",
        "]\n",
        "\n",
        "**Where:**\n",
        "\n",
        "* ( p_i ) = probability of each class in the node.\n",
        "\n",
        "**Range:** 0 to 0.5 (for binary classification)\n",
        "\n",
        "* 0 → perfect purity\n",
        "* Higher value → more impurity\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "* Simpler and faster to compute than entropy.\n",
        "* Often used in the **CART (Classification and Regression Tree)** algorithm.\n",
        "\n",
        "\n",
        "### **2. Entropy**\n",
        "\n",
        "**Definition:**\n",
        "Entropy measures the amount of **information (or uncertainty)** in the data. It quantifies the disorder or randomness in the dataset.\n",
        "\n",
        "**Formula:**\n",
        "[\n",
        "Entropy = -\\sum p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "**Where:**\n",
        "\n",
        "* ( p_i ) = probability of each class in the node.\n",
        "\n",
        "**Range:** 0 to 1 (for binary classification)\n",
        "\n",
        "* 0 → perfectly pure node\n",
        "* 1 → maximum impurity\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "* Based on the concept of **information theory**.\n",
        "* Used in algorithms such as **ID3** and **C4.5**.\n",
        "\n",
        "\n",
        "### **3. Comparison Table**\n",
        "\n",
        "| **Aspect**              | **Gini Impurity**                                         | **Entropy**                                                                         |\n",
        "| ----------------------- | --------------------------------------------------------- | ----------------------------------------------------------------------------------- |\n",
        "| **Meaning**             | Measures the probability of incorrect classification.     | Measures the amount of information or disorder.                                     |\n",
        "| **Formula**             | ( Gini = 1 - \\sum p_i^2 )                                 | ( Entropy = -\\sum p_i \\log_2(p_i) )                                                 |\n",
        "| **Range (Binary Case)** | 0 to 0.5                                                  | 0 to 1                                                                              |\n",
        "| **Computation Speed**   | Faster (no logarithm involved)                            | Slower (requires logarithmic calculations)                                          |\n",
        "| **Interpretation**      | Focuses on misclassification probability.                 | Focuses on information content.                                                     |\n",
        "| **When to Use**         | Preferred when computational efficiency is needed (CART). | Preferred when interpretability based on information gain is important (ID3, C4.5). |\n",
        "| **Behavior**            | Slightly favors larger partitions.                        | Tends to create more balanced trees.                                                |\n",
        "\n",
        "### **4. Example:**\n",
        "\n",
        "If a node has two classes with probabilities:\n",
        "\n",
        "* Class A: 0.8\n",
        "* Class B: 0.2\n",
        "\n",
        "Then,\n",
        "[\n",
        "Gini = 1 - (0.8^2 + 0.2^2) = 0.32\n",
        "]\n",
        "[\n",
        "Entropy = -[0.8 \\log_2(0.8) + 0.2 \\log_2(0.2)] = 0.72\n",
        "]\n",
        "\n",
        "Both indicate that the node is **not pure**, but **entropy** gives a higher impurity value.\n",
        "\n",
        "### **5. Conclusion:**\n",
        "\n",
        "* **Gini Impurity** and **Entropy** both serve the same purpose — to measure node impurity and help in selecting the best feature for splitting.\n",
        "* **Gini Impurity** is **computationally simpler** and preferred for speed, while **Entropy** provides a **more theoretical and information-based measure** of impurity.\n",
        "* In practice, both yield **similar results**, and the choice between them depends on the algorithm used or computational preferences.\n"
      ],
      "metadata": {
        "id": "OAGAHjjR3rkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "\n",
        "### **Definition:**\n",
        "\n",
        "**Pre-Pruning**, also known as **Early Stopping**, is a technique used in **Decision Tree learning** to **prevent overfitting** by **stopping the tree from growing too deep or too complex** during its construction.\n",
        "Instead of allowing the tree to expand fully and then trimming it later, pre-pruning places **restrictions** while the tree is being built.\n",
        "\n",
        "### **Purpose:**\n",
        "\n",
        "The main goal of pre-pruning is to **avoid unnecessary splits** that do not significantly improve the model’s accuracy and to **reduce model complexity**.\n",
        "This helps in building a **simpler, faster, and more generalizable** tree.\n",
        "\n",
        "\n",
        "### **How It Works:**\n",
        "\n",
        "During the construction of the Decision Tree, the algorithm evaluates whether a **split should be made** based on certain criteria.\n",
        "If the criteria are **not met**, the split is **not performed**, and the node is turned into a **leaf node**.\n",
        "\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Suppose we are training a decision tree to predict whether a student passes or fails based on study hours and attendance.\n",
        "\n",
        "If a potential split increases accuracy by only **0.5%**, pre-pruning may **stop** this split because the improvement is not significant — helping the model stay **simple and general**.\n",
        "\n",
        "### **Advantages:**\n",
        "\n",
        "* Prevents **overfitting** during training.\n",
        "* Produces a **smaller and faster** model.\n",
        "* Reduces computational time and resources.\n",
        "\n",
        "### **Disadvantages:**\n",
        "\n",
        "* May cause **underfitting** if the pruning threshold is too strict.\n",
        "* The model might miss useful patterns in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "LvvqKN4f4BlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n"
      ],
      "metadata": {
        "id": "YWKe7Tco4Vuy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pyO9mOv3UA7",
        "outputId": "2ac0ee6c-f9f7-4e98-935b-02e20520de3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n",
            "\n",
            "Predicted Class for Sample: setosa\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data                # Features\n",
        "y = data.target              # Target variable\n",
        "\n",
        "# Create a Decision Tree Classifier using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train (fit) the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature names and their importance scores\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Predict for a new sample (optional)\n",
        "sample = [[5.0, 3.6, 1.4, 0.2]]\n",
        "prediction = model.predict(sample)\n",
        "print(\"\\nPredicted Class for Sample:\", data.target_names[prediction[0]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Explanation:**\n",
        "\n",
        "* **criterion='gini'** → Tells the classifier to use **Gini Impurity** for measuring splits.\n",
        "* **model.fit(X, y)** → Trains the model using the dataset.\n",
        "* **.feature_importances_** → Shows how much each feature contributes to the decision-making process.\n",
        "* Features with higher importance have a **stronger impact** on predictions.\n",
        "\n",
        "\n",
        "### **Interpretation of Output:**\n",
        "\n",
        "In this example:\n",
        "\n",
        "* **Petal length** and **petal width** are the most important features for classifying Iris flowers.\n",
        "* **Sepal length** and **sepal width** have minimal contribution.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hal0_r0x4cVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "**Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification** and **regression**.\n",
        "It works by finding the **best boundary (hyperplane)** that separates data points of different classes with the **maximum margin**.\n",
        "\n",
        "### **Key Terms:**\n",
        "\n",
        "* **Hyperplane:** The decision boundary separating classes.\n",
        "* **Support Vectors:** Data points closest to the hyperplane; they define the boundary.\n",
        "* **Margin:** Distance between the hyperplane and the nearest data points — SVM maximizes this distance for better accuracy.\n",
        "\n",
        "### **Equation:**\n",
        "\n",
        "w · x + b = 0\n",
        "\n",
        "Where\n",
        "\n",
        "* *w* = weights,\n",
        "* *x* = input features,\n",
        "* *b* = bias term.\n",
        "\n",
        "### **Types:**\n",
        "\n",
        "* **Linear SVM:** For linearly separable data.\n",
        "* **Non-linear SVM:** Uses **kernels** (e.g., RBF, polynomial) to handle complex data.\n"
      ],
      "metadata": {
        "id": "l1KgwAtm4ikq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "### **Definition:**\n",
        "\n",
        "The **Kernel Trick** in **Support Vector Machines (SVM)** is a mathematical technique that allows SVMs to **separate non-linearly separable data** by **transforming it into a higher-dimensional space** — **without explicitly performing the transformation**.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "> The Kernel Trick lets SVM find a separating hyperplane in higher dimensions **without increasing computational cost**.\n",
        "\n",
        "\n",
        "### **Purpose:**\n",
        "\n",
        "Many datasets cannot be separated by a straight line (linear boundary).\n",
        "The Kernel Trick helps convert such **non-linear problems** into **linearly separable ones** by applying a **kernel function**.\n",
        "\n",
        "\n",
        "### **How It Works:**\n",
        "\n",
        "* Instead of mapping data points ( x ) into higher dimensions directly using a function ( \\phi(x) ),\n",
        "  SVM uses a **kernel function ( K(x_i, x_j) )** that computes the **dot product** in that higher-dimensional space:\n",
        "K(x_i, x_j) = φ(x_i) · φ(x_j)\n",
        "\n",
        "* This allows complex transformations **implicitly**, saving time and memory.\n",
        "\n",
        "\n",
        "Common Kernel Functions:\n",
        "\n",
        "1. Linear Kernel:      K(x_i, x_j) = x_i · x_j\n",
        "   → Used when data is linearly separable.\n",
        "\n",
        "2. Polynomial Kernel:  K(x_i, x_j) = (x_i · x_j + 1)^d\n",
        "   → Used when data has curved or polynomial relationships.\n",
        "\n",
        "3. RBF Kernel:         K(x_i, x_j) = e^(-γ ||x_i - x_j||²)\n",
        "   → Used for complex, non-linear data.\n",
        "\n",
        "4. Sigmoid Kernel:     K(x_i, x_j) = tanh(α(x_i · x_j) + c)\n",
        "   → Similar to neural network activation function.\n",
        "\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "If two classes can’t be separated by a straight line in 2D,\n",
        "the **RBF kernel** maps them into 3D, where a **plane** can easily divide them.\n",
        "\n"
      ],
      "metadata": {
        "id": "eMYZylu443um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "GwnLsEtj5TwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data          # Features\n",
        "y = data.target        # Target labels\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create two SVM classifiers — one Linear, one RBF\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train both models\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for both models\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of SVM with Linear Kernel: {:.2f}%\".format(acc_linear * 100))\n",
        "print(\"Accuracy of SVM with RBF Kernel: {:.2f}%\".format(acc_rbf * 100))\n",
        "\n",
        "# Compare results\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\nLinear Kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"\\nRBF Kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\nBoth kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mscDNBPU5Zz7",
        "outputId": "6c5b4494-d38b-41b9-ccd0-bc665f78dc68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 100.00%\n",
            "Accuracy of SVM with RBF Kernel: 80.56%\n",
            "\n",
            "Linear Kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "SVC(kernel='linear') → Creates an SVM that uses a linear decision boundary.\n",
        "\n",
        "SVC(kernel='rbf') → Uses a non-linear (RBF) kernel that maps data into higher dimensions.\n",
        "\n",
        "accuracy_score() → Measures how well each model performs on unseen data."
      ],
      "metadata": {
        "id": "W6m0wcjI5a7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "### **Definition:**\n",
        "\n",
        "The **Naïve Bayes classifier** is a **supervised machine learning algorithm** based on **Bayes’ Theorem**, used mainly for **classification tasks**.\n",
        "It predicts the probability that a data point belongs to a particular class given its features.\n",
        "\n",
        "It is especially popular for **text classification**, **spam filtering**, and **sentiment analysis** because of its **simplicity and efficiency**.\n",
        "\n",
        "\n",
        "### **Bayes’ Theorem Formula:**\n",
        "\n",
        "```\n",
        "P(A|B) = [ P(B|A) * P(A) ] / P(B)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* **P(A|B)** → Probability of class *A* given the data *B* (posterior probability)\n",
        "* **P(B|A)** → Probability of data *B* given class *A* (likelihood)\n",
        "* **P(A)** → Prior probability of class *A*\n",
        "* **P(B)** → Probability of observing the data *B*\n",
        "\n",
        "### **Why It’s Called “Naïve”:**\n",
        "\n",
        "It is called **“Naïve”** because the algorithm **assumes that all features (predictors) are independent** of each other, given the class label.\n",
        "\n",
        "In real-world data, this assumption is rarely true — features often influence each other — but the classifier still performs surprisingly well in practice.\n",
        "\n",
        "**Example:**\n",
        "While predicting if an email is spam, Naïve Bayes assumes that the presence of words like *“offer”* and *“discount”* are **independent** events, even though they are often related.\n",
        "\n",
        "### **Key Characteristics:**\n",
        "\n",
        "* **Simple and fast** to train.\n",
        "* Works well with **high-dimensional data** (like text).\n",
        "* Requires a **small amount of training data**.\n",
        "* Performs well even when the independence assumption is not perfectly true.\n",
        "\n",
        "### **Types of Naïve Bayes Classifiers:**\n",
        "\n",
        "| **Type**                    | **Description**                                                        |\n",
        "| --------------------------- | ---------------------------------------------------------------------- |\n",
        "| **Gaussian Naïve Bayes**    | Used when features are continuous and normally distributed.            |\n",
        "| **Multinomial Naïve Bayes** | Used for discrete features (e.g., word counts in text classification). |\n",
        "| **Bernoulli Naïve Bayes**   | Used for binary features (e.g., word presence or absence).             |\n"
      ],
      "metadata": {
        "id": "pagrJH645fsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "### **Introduction:**\n",
        "\n",
        "The **Naïve Bayes algorithm** has several variants depending on the **type of data** it handles.\n",
        "All are based on **Bayes’ Theorem**, but they differ in how they **model the distribution of features**.\n",
        "\n",
        "\n",
        "### **1. Gaussian Naïve Bayes (GNB)**\n",
        "\n",
        "**Used For:**\n",
        "Continuous numerical features (e.g., age, height, temperature).\n",
        "\n",
        "**Assumption:**\n",
        "The features follow a **normal (Gaussian) distribution**.\n",
        "\n",
        "**Probability Model:**\n",
        "P(x_i|y) = (1 / √(2πσ_y²)) * e^(-(x_i - μ_y)² / (2σ_y²))\n",
        "\n",
        "**Example Use Cases:**\n",
        "\n",
        "* Medical data (like predicting disease risk from blood pressure or glucose levels).\n",
        "* Sensor readings or continuous measurements.\n",
        "\n",
        "\n",
        "### **2. Multinomial Naïve Bayes (MNB)**\n",
        "\n",
        "**Used For:**\n",
        "Discrete or count-based features (e.g., word frequencies in text).\n",
        "\n",
        "**Assumption:**\n",
        "Features represent **the frequency or count** of events and follow a **Multinomial distribution**.\n",
        "\n",
        "**Probability Model (simplified):**\n",
        "P(x_i|y) = [(N_y)! / (x_{i1}! x_{i2}! ... x_{in}!)] * Π_j (p_yj)^(x_ij)\n",
        "\n",
        "**Example Use Cases:**\n",
        "\n",
        "* Text classification\n",
        "* Spam detection\n",
        "* Document categorization\n",
        "\n",
        "\n",
        "### **3. Bernoulli Naïve Bayes (BNB)**\n",
        "\n",
        "**Used For:**\n",
        "Binary or boolean features (e.g., word presence or absence).\n",
        "\n",
        "**Assumption:**\n",
        "Each feature is **binary** — it can take only **two values** (0 or 1).\n",
        "\n",
        "**Probability Model (simplified):**\n",
        "P(x_i|y) = (p_y)^(x_i) * (1 - p_y)^(1 - x_i)\n",
        "\n",
        "\n",
        "**Example Use Cases:**\n",
        "\n",
        "* Sentiment analysis (word present = 1, absent = 0)\n",
        "* Email spam filtering (specific keywords present or not)\n",
        "\n",
        "### **Comparison Table**\n",
        "\n",
        "| **Aspect**                   | **Gaussian NB**           | **Multinomial NB**                | **Bernoulli NB**                     |\n",
        "| ---------------------------- | ------------------------- | --------------------------------- | ------------------------------------ |\n",
        "| **Feature Type**             | Continuous                | Discrete (counts)                 | Binary (0 or 1)                      |\n",
        "| **Distribution Assumed**     | Normal (Gaussian)         | Multinomial                       | Bernoulli                            |\n",
        "| **Input Example**            | [5.1, 3.5, 1.4]           | [2, 0, 3, 1]                      | [1, 0, 1, 0]                         |\n",
        "| **Used In**                  | Medical data, sensor data | Text classification (word counts) | Binary text features (word presence) |\n",
        "| **Common Libraries**         | `GaussianNB()`            | `MultinomialNB()`                 | `BernoulliNB()`                      |\n",
        "| **From sklearn.naive_bayes** | Yes                         | Yes                                 | Yes                                    |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1r_ohRfz5zXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 10: Breast Cancer Dataset Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGluILHO6XIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data         # Features\n",
        "y = data.target       # Target labels (0 = malignant, 1 = benign)\n",
        "\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train (fit) the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Gaussian Naïve Bayes Classifier: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LECWz6Na6q-2",
        "outputId": "bd3a3462-1fcd-41e0-d622-eb85c18cefdd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes Classifier: 97.37%\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "GaussianNB() → Applies the Gaussian Naïve Bayes algorithm, assuming features follow a normal distribution.\n",
        "\n",
        "fit() → Trains the model using the training dataset.\n",
        "\n",
        "predict() → Predicts the class labels for the test data.\n",
        "\n",
        "accuracy_score() → Measures the percentage of correct predictions.\n",
        "\n",
        "classification_report() → Provides detailed performance metrics (precision, recall, F1-score)."
      ],
      "metadata": {
        "id": "mXVLyeE06uuM"
      }
    }
  ]
}